{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project Road Segmentation Using  Tensor Flow\n",
    "\n",
    "\n",
    "> We import the required python library for the homework below. Note that we have imported a python file(ourProjectFunctions) that contains some functions we would define ourselves. Those functions will be explained in the appropriate palces before it is to be used in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import ourProjectFunctions     # Python Function file with the functions we will use in the project. \n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 666 # For the submission\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 800 images\n",
      "satImage_001.png\n",
      "Loading 800 images\n",
      "satImage_001.png\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"..\\\\training\\\\\"\n",
    "\n",
    "image_dir = root_dir + \"images_ext\\\\\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(10000, len(files)) # Load  100 images. \n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [ourProjectFunctions.load_image(image_dir + files[i], convert_lab=True) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth_ext\\\\\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [ourProjectFunctions.load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth and shape of Satellite  images after patching and  Linearisation: \n",
      "(2000000, 8, 8, 3)\n",
      "lenth and shape of Ground truth  images after patching and Linearisation: \n",
      "(2000000, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 8\n",
    "\n",
    "img_patches = [ourProjectFunctions.img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [ourProjectFunctions.img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "X = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "\n",
    "print (\"lenth and shape of Satellite  images after patching and  Linearisation: \")\n",
    "print(X.shape)\n",
    "print (\"lenth and shape of Ground truth  images after patching and Linearisation: \")\n",
    "print(gt_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = np.asarray([ourProjectFunctions.value_to_class_for_tensor_flow(np.mean(gt_patches[i])) for i in range(len(gt_patches))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(1999800,8)\n",
      "validation_images(200,8)\n"
     ]
    }
   ],
   "source": [
    "#img_size=10;\n",
    "VALIDATION_SIZE = 200\n",
    "label_count = 2;\n",
    "validation_images = X[:VALIDATION_SIZE]\n",
    "validation_labels = Y[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = X[VALIDATION_SIZE:]\n",
    "train_labels = Y[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999800, 8, 8, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 8, 8, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_count = 2\n",
    "data_nodes = tf.placeholder('float', shape=[None, patch_size, patch_size, 3])\n",
    "label_nodes = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed=SEED)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 32)\n"
     ]
    }
   ],
   "source": [
    "# first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# (40000,784) => (40000,28,28,1)\n",
    "#image = tf.reshape(X, [-1,image_width , image_height,1])\n",
    "#print (image.get_shape()) # =>(40000,28,28,1)\n",
    "\n",
    "\n",
    "h_conv1 = tf.nn.relu6(tf.nn.bias_add(conv2d(data_nodes, W_conv1), b_conv1))\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "print (h_pool1.get_shape()) # => (40000, 14, 14, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "#2nd convolution layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 128])\n",
    "b_conv2 = bias_variable([128])\n",
    "\n",
    "h_conv2 = tf.nn.relu6(tf.nn.bias_add(conv2d(h_pool1, W_conv2), b_conv2))\n",
    "#print (h_conv2.get_shape()) # => (128000, 14,14, 64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "print (h_pool2.get_shape()) # => (1280000, 7, 7, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2048)\n"
     ]
    }
   ],
   "source": [
    "# densely connected layer\n",
    "W_fc1 = weight_variable([int(patch_size/4 * patch_size/4 * 128), 2048])\n",
    "b_fc1 = bias_variable([2048])\n",
    "\n",
    "# (40000, 7, 7, 64) => (40000, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, int(patch_size/4 * patch_size/4 * 128)])\n",
    "\n",
    "h_fc1 = tf.nn.relu6(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "print (h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# readout layer for deep net\n",
    "labels_count =2\n",
    "W_fc2 = weight_variable([2048, labels_count])\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "\n",
    "y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# cost function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, label_nodes))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(b_fc1) + tf.nn.l2_loss(W_fc2) + tf.nn.l2_loss(b_fc2)\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy + 5e-6 * regularizers)\n",
    "# train_step = tf.train.MomentumOptimizer(0.001, 0.0).minimize(cross_entropy)\n",
    "# train_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y),1), tf.argmax(label_nodes,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = tf.argmax(tf.nn.softmax(y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 200000\n",
    "    \n",
    "DROPOUT = 0.5\n",
    "#BATCH_SIZES = 200\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZES = 2000\n",
    "BATCH_SIZE= 200\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_validation_stats():\n",
    "    validation_accuracy = accuracy.eval(feed_dict={data_nodes: validation_images, \n",
    "                                                   label_nodes: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.3)\n",
    "   \n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions():\n",
    "    # Load submission images\n",
    "    test_dir = '..\\\\test_set_images\\\\'\n",
    "    submission_dir = '..\\\\submission\\\\'\n",
    "    overlay_dir = '..\\\\submission_overlay\\\\'\n",
    "\n",
    "    if not os.path.isdir(submission_dir):\n",
    "        os.mkdir(submission_dir)\n",
    "\n",
    "    if not os.path.isdir(overlay_dir):\n",
    "        os.mkdir(overlay_dir)\n",
    "\n",
    "    files = os.listdir(test_dir)\n",
    "\n",
    "    for file in files:\n",
    "        img = ourProjectFunctions.load_image(test_dir + file + '\\\\' + file + '.png', convert_lab=True)\n",
    "        img_patches = ourProjectFunctions.img_crop(img, patch_size, patch_size)\n",
    "        X = np.array(img_patches)\n",
    "\n",
    "        prediction = sess.run(predict, feed_dict={ data_nodes: X, keep_prob: 1.0 })\n",
    "        img_prediction = ourProjectFunctions.label_to_img(608, 608, patch_size, patch_size, prediction)\n",
    "\n",
    "        img_overlay = ourProjectFunctions.make_img_overlay(img, img_prediction)\n",
    "\n",
    "        save_path = file + \".png\"\n",
    "        Image.fromarray(ourProjectFunctions.img_float_to_uint8(img_prediction)).save(submission_dir + save_path)\n",
    "        Image.fromarray(ourProjectFunctions.img_float_to_uint8(img_overlay)).save(overlay_dir + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver to be able to restore a model\n",
    "saver = tf.train.Saver()\n",
    "save_dir = '..\\\\tmp\\\\'\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.09 / 0.10 for step 0\n",
      "training_accuracy / validation_accuracy => 0.52 / 0.10 for step 1\n",
      "training_accuracy / validation_accuracy => 0.14 / 0.14 for step 2\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.90 for step 3\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 4\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.90 for step 5\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.90 for step 6\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 7\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 8\n",
      "training_accuracy / validation_accuracy => 0.58 / 0.90 for step 9\n",
      "training_accuracy / validation_accuracy => 0.80 / 0.90 for step 10\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 20\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.90 for step 30\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.90 for step 40\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.90 for step 50\n",
      "training_accuracy / validation_accuracy => 0.80 / 0.90 for step 60\n",
      "training_accuracy / validation_accuracy => 0.85 / 0.90 for step 70\n",
      "training_accuracy / validation_accuracy => 0.54 / 0.90 for step 80\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.90 for step 90\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 100\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.72 for step 200\n",
      "training_accuracy / validation_accuracy => 0.71 / 0.88 for step 300\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.94 for step 400\n",
      "training_accuracy / validation_accuracy => 0.65 / 0.90 for step 500\n",
      "training_accuracy / validation_accuracy => 0.31 / 0.81 for step 600\n",
      "training_accuracy / validation_accuracy => 0.55 / 0.90 for step 700\n",
      "training_accuracy / validation_accuracy => 0.86 / 0.94 for step 800\n",
      "training_accuracy / validation_accuracy => 0.78 / 0.94 for step 900\n",
      "training_accuracy / validation_accuracy => 0.94 / 0.92 for step 1000\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.90 for step 2000\n",
      "training_accuracy / validation_accuracy => 0.51 / 0.90 for step 3000\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.94 for step 4000\n",
      "training_accuracy / validation_accuracy => 1.00 / 0.95 for step 5000\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.92 for step 6000\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.92 for step 7000\n",
      "training_accuracy / validation_accuracy => 0.82 / 0.94 for step 8000\n",
      "training_accuracy / validation_accuracy => 0.45 / 0.89 for step 9000\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.89 for step 10000\n",
      "training_accuracy / validation_accuracy => 0.80 / 0.95 for step 20000\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.93 for step 30000\n",
      "training_accuracy / validation_accuracy => 0.86 / 0.94 for step 40000\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.94 for step 50000\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.93 for step 60000\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.93 for step 70000\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.94 for step 80000\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.93 for step 90000\n",
      "training_accuracy / validation_accuracy => 0.85 / 0.92 for step 100000\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.94 for step 199999\n",
      "validation_accuracy => 0.9450\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYqYwERGRmClMREQkZv8fcqVag3Fr56QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x285509dfc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "image_size = 400;\n",
    "\n",
    "display_step=1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "        #get new batch\n",
    "        batch_xs, batch_ys = next_batch(BATCH_SIZE)  \n",
    "\n",
    "\n",
    "        # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "        if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "\n",
    "            train_accuracy = accuracy.eval(feed_dict={data_nodes: batch_xs, \n",
    "                                                      label_nodes: batch_ys, \n",
    "                                                      keep_prob: 1.0})       \n",
    "            if(VALIDATION_SIZE):\n",
    "                validation_accuracy = accuracy.eval(feed_dict={ data_nodes: validation_images, \n",
    "                                                                label_nodes: validation_labels, \n",
    "                                                                keep_prob: 1.0})                                  \n",
    "                print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d' % \n",
    "                      (train_accuracy, validation_accuracy, i))\n",
    "\n",
    "                validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "            else:\n",
    "                 print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            x_range.append(i)\n",
    "\n",
    "            # increase display_step\n",
    "            if i%(display_step*10) == 0 and i:\n",
    "                display_step *= 10\n",
    "        \n",
    "        # train on batch\n",
    "        sess.run(train_step, feed_dict={data_nodes: batch_xs, label_nodes: batch_ys, keep_prob: DROPOUT})\n",
    "    \n",
    "    display_validation_stats()\n",
    "    \n",
    "    generate_predictions()\n",
    "    \n",
    "    saver.save(sess, '..\\\\tmp\\\\model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        generate_predictions()\n",
    "    else:\n",
    "        print('Oops, cannot load the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
