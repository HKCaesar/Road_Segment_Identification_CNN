{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project Road Segmentation Using  Tensor Flow\n",
    "\n",
    "\n",
    "> We import the required python library for the homework below. Note that we have imported a python file(ourProjectFunctions) that contains some functions we would define ourselves. Those functions will be explained in the appropriate palces before it is to be used in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import ourProjectFunctions     # Python Function file with the functions we will use in the project. \n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 666 # For the submission\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "satImage_001.png\n",
      "Loading 100 images\n",
      "satImage_001.png\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"..\\\\training\\\\\"\n",
    "\n",
    "image_dir = root_dir + \"images\\\\\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(10000, len(files)) # Load  100 images. \n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [ourProjectFunctions.load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth\\\\\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [ourProjectFunctions.load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth and shape of Satellite  images after patching and  Linearisation: \n",
      "62500\n",
      "(62500, 16, 16, 3)\n",
      "lenth and shape of Ground truth  images after patching and Linearisation: \n",
      "62500\n",
      "(62500, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 16# each patch is 16*16 pixels\n",
    "\n",
    "img_patches = [ourProjectFunctions.img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [ourProjectFunctions.img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "# Linearize list of patches\n",
    "X = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "print (\"lenth and shape of Satellite  images after patching and  Linearisation: \")\n",
    "print(len(X))\n",
    "print(X.shape)\n",
    "print (\"lenth and shape of Ground truth  images after patching and Linearisation: \")\n",
    "print(len(gt_patches))\n",
    "print(gt_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = np.asarray([ourProjectFunctions.value_to_class_for_tensor_flow(np.mean(gt_patches[i])) for i in range(len(gt_patches))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(60500,16)\n",
      "validation_images(2000,16)\n"
     ]
    }
   ],
   "source": [
    "#img_size=10;\n",
    "VALIDATION_SIZE = 2000\n",
    "label_count = 2;\n",
    "validation_images = X[:VALIDATION_SIZE]\n",
    "validation_labels = Y[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = X[VALIDATION_SIZE:]\n",
    "train_labels = Y[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60500, 16, 16, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 16, 16, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels_count =2\n",
    "x = tf.placeholder('float', shape=[None,10,10,3])\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_count =2\n",
    "x = tf.placeholder('float', shape=[None,16,16,3])\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed=SEED)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 32)\n"
     ]
    }
   ],
   "source": [
    "# first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# (40000,784) => (40000,28,28,1)\n",
    "#image = tf.reshape(X, [-1,image_width , image_height,1])\n",
    "#print (image.get_shape()) # =>(40000,28,28,1)\n",
    "\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "print (h_pool1.get_shape()) # => (40000, 14, 14, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "#2nd convolution layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#print (h_conv2.get_shape()) # => (128000, 14,14, 64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "print (h_pool2.get_shape()) # => (1280000, 7, 7, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Change this to code cell from menu bar to run this cell. This is if we define patch size as 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1024)\n"
     ]
    }
   ],
   "source": [
    "# densely connected layer\n",
    "W_fc1 = weight_variable([4* 4 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# (40000, 7, 7, 64) => (40000, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*64])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "print (h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# readout layer for deep net\n",
    "labels_count =2\n",
    "W_fc2 = weight_variable([1024, labels_count])\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "print (y.get_shape()) # => (40000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = tf.argmax(y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 50000\n",
    "    \n",
    "DROPOUT = 0.5\n",
    "#BATCH_SIZES = 200\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZES = 2000\n",
    "BATCH_SIZE= 200\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saver to be able to restore a model\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start TensorFlow session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.44 / 0.44 for step 0\n",
      "training_accuracy / validation_accuracy => 0.62 / 0.71 for step 1\n",
      "training_accuracy / validation_accuracy => 0.70 / 0.73 for step 2\n",
      "training_accuracy / validation_accuracy => 0.74 / 0.73 for step 3\n",
      "training_accuracy / validation_accuracy => 0.59 / 0.73 for step 4\n",
      "training_accuracy / validation_accuracy => 0.95 / 0.73 for step 5\n",
      "training_accuracy / validation_accuracy => 0.61 / 0.73 for step 6\n",
      "training_accuracy / validation_accuracy => 0.76 / 0.73 for step 7\n",
      "training_accuracy / validation_accuracy => 0.57 / 0.73 for step 8\n",
      "training_accuracy / validation_accuracy => 0.68 / 0.70 for step 9\n",
      "training_accuracy / validation_accuracy => 0.67 / 0.66 for step 10\n",
      "training_accuracy / validation_accuracy => 0.54 / 0.73 for step 20\n",
      "training_accuracy / validation_accuracy => 0.40 / 0.73 for step 30\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.73 for step 40\n",
      "training_accuracy / validation_accuracy => 0.65 / 0.73 for step 50\n",
      "training_accuracy / validation_accuracy => 0.79 / 0.73 for step 60\n",
      "training_accuracy / validation_accuracy => 0.38 / 0.73 for step 70\n",
      "training_accuracy / validation_accuracy => 0.76 / 0.73 for step 80\n",
      "training_accuracy / validation_accuracy => 0.54 / 0.75 for step 90\n",
      "training_accuracy / validation_accuracy => 0.67 / 0.73 for step 100\n"
     ]
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "image_size = 400;\n",
    "\n",
    "display_step=1\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)  \n",
    "    \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_images, \n",
    "                                                            y_: validation_labels, \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ckpt = tf.train.get_checkpoint_state('..\\\\tf_model\\\\')\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print('Oops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_accuracy => 0.8135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VPXZ//H3nYUt7ItIFEEqoFZFXFAr2qgtKlq1tSqg\niH1a62OrUq0/l1orLmh56laqttJaF1Cx2hZRQNDW4FYVFVQokCiKbIpAQNmz3L8/vpMwCVmGIScz\nST6v65orZ856z4HMne96zN0RERHZVRmpDkBERBonJRAREUmKEoiIiCRFCURERJKiBCIiIklRAhER\nkaREmkDM7CEz+8LMPqhhe38ze8PMtprZVVHGIiIi9SvqEsjDwMm1bF8LXA78LuI4RESknkWaQNz9\nNaColu1r3P1doCTKOEREpP6pDURERJKiBCIiIknJSnUAiTIzTdolIpIEd7coztsQJRCLvRLZr1bu\nrpc7N910U8pjSJeX7oXuhe5F7a8oRVoCMbMngDygi5l9BtwEtADc3SeYWXfgHaAdUGZmo4ED3X1j\nlHGJiMjuizSBuPuIOrZ/AfSMMgYREYmGGtEboby8vFSHkDZ0L3bQvdhB96JhWNR1ZPXFzLyxxCoi\nki7MDG/EjegiItIEKYGIiEhSlEBERCQpSiAiIpIUJRAREUmKEoiIiCRFCURERJKiBCIiIklRAhER\nkaQogYiISFKUQEREJClKICIikhQlEBERSYoSiIiIJEUJREREkqIEIiIiSVECERGRpESaQMzsITP7\nwsw+qGWf8WZWaGbzzOzQKOMREZH6E3UJ5GHg5Jo2mtmpwDfcvS9wCfCniOMREZF6EmkCcffXgKJa\ndjkTeCy271tABzPrHmVMIiJSP1LdBrIXsCzu/YrYOhERSXOpTiAN5rbb4LHHUh1F07BtGwwYAC+9\nlOpIRCSVslJ8/RVAz7j3e8fWVWvMmDEVy3l5eeTl5SV8ocLCXY5NavDooyGJjBoF8+ZBt26pjkhE\nyuXn55Ofn98g1zJ3j/YCZr2B59z94Gq2DQV+7u6nmdnRwL3ufnQN5/HdifWMM+Cgg+D225M+hQAl\nJdC/f0gi06bBBx/Ac89BRrMpy4o0LmaGu1sU5466G+8TwBtAPzP7zMx+ZGaXmNlPAdx9OvCJmX0E\nPAj8LKpY1q+HTZuiOnvzMXky7L03DB4Mt9wCa9bA+PGpjkpEUiHSKix3H5HAPpdFGUO5oiLYvLkh\nrtR0lZXBHXfAPfeE99nZ8OSTcNRRcPzxcNhhqY1PRBpWs6l4UAlk902ZAm3awHe/u2Ndnz6hBDJ8\nOGzcmLrYRKThNZsEohLI7nGHsWPhhhvAqtSmDh8O3/oWXHFFamITkdRoFgmkuDiUPlQCSd6sWaHn\n1RlnVL/9D3+A118PVVoi0jw0iwSyfn34qQSSvLFj4Ve/qrm3Vdu2oYH9iitgyZKGjU1EUqNZJJCi\n2GQqqsJKzquvwooVcO65te83cGCo4ho+PJT6RKRpaxYJZP16aN9eJZBkjR0L110HWQn02Rs9Grp2\nhd/8Jvq4RCS1mkUCKSqCvfZSCSQZ77wDCxbAhRcmtr8ZPPxwmDZGU52ING3NJoHsvbdKIMm4/Xa4\n+mpo2TLxY/bYIySQUaNg9eroYhOR1GoWCWT9+lACUQLZNQsWwBtvwMUX7/qxJ50USi0XXRQGIIpI\n09MsEkhREXTvHsYyqHE3cXfcEdo02rRJ7vhbboG1azXViUhT1WwSSKdO4YtQpZDELFkCL7wAP9uN\n2cnKpzoZOxbee6/+YhOR9NAsEsj69dCxI+TkqCE9UePGwaWXQocOu3ceTXUi0nQ1iwSiEsiuWbEC\nnn46VF/Vh+HD4dhj4fLL6+d8IpIemkUCWb8+JBCVQBJz552h8btr1/o75/jxoUH+iSfq75wiklqp\nfiJhgygq2lGFpRJI7b78Mjwsav78+j1v+VQnQ4bA0UeHqi0RadyaRQlEVViJu/feMGVJbm79n1tT\nnYg0Lc0igagRPTHr18ODD8K110Z3jfKpTm68MbpriEjDaPIJpKwMNmwICUQlkNrdfz8MHQr77hvd\nNczgkUdg4kR48cXoriMi0WvybSAbN4bEkZWlEkhtNm0KDd35+dFfq1u3MNXJhRfC3Llh6hMRaXwi\nL4GY2SlmtsjMCsxsp8oRM+toZv8ws/fN7E0zO7A+r1/egA5qRK/Nn/8Mxx0HBxzQMNfTVCcijV+k\nCcTMMoD7gJOBbwLDzWz/Krv9Cpjr7gOAUUC9TnxR3oAOqsKqybZtoevuDTc07HU11YlI4xZ1CWQQ\nUOjuS929GJgMnFllnwOBfwO4+2Kgt5l1q68AyhvQQVVYNXn0UTjkkNBLqiFpqhORxi3qBLIXsCzu\n/fLYunjvAz8AMLNBwD7A3vUVQHwJRFVYOyspCdOW/OpXqbl++VQnw4ZpqhORxiYdGtF/C/zezN4D\nPgTmAqXV7ThmzJiK5by8PPLy8uo8efkodAhVWCqBVDZ5cnhWyuDBqYth+PDQI+vyy8PDqEQkefn5\n+eQ3RG8YwNw9upObHQ2McfdTYu+vA9zdx9VyzCfAwe6+scp6TybWu++GZcvgnntC19GZM2HSpF0+\nTZNUVgYHHxzuzZAhqY1l40Y4/HC46SYYMSK1sYg0JWaGu1sU5466CmsOsJ+Z9TKzFsAwYGr8DmbW\nwcyyY8sXA7OrJo/dUbURXSWQHaZMCffku99NdSQ7pjoZPTpMJS8i6S/SBOLupcBlwCxgATDZ3Rea\n2SVm9tPYbgcA881sIaG3Vj3NARtUbURXG0jgHh5Xe8MNYXBfOtBUJyKNS+RtIO7+AtC/yroH45bf\nrLq9PqkRvXqzZsHWrXDGGamOpLLRo+Gll8JUJ7/9baqjEZHaNPmpTFSFVb2xY0PPq4w0+x9gFhrS\nNdWJSPpLs6+P+qcqrJ29+iqsXBlm3U1H5VOdXHQRrF6d6mhEpCZNPoGoBLKzsWPDjLtZ6dCJuwaa\n6kQk/TX5BKISSGXvvAMLFoQv53R3yy2wbh38/vepjkREqtOoEkhh4a4fU7URvbmXQG6/Ha6+Glq2\nTHUkdSuf6uT22zXViUg6inQgYX0yMwdnV8LduhU6dAg/zULX1aws2L4dMjOjizVdLVgQqoaWLAnV\neY3Fk0+GAYbvvRfGi4hI4hrzQMKUKq++Kh/nYNa8Z+T97W9DN9nGlDwgjAsZPDhMdSIidVuzJswx\nd9hh0V6nSSeQ+Oqrcs21IX3JEpgxA372s1RHkpzx4+GNN+CJJ1IdiUh6Ki6G556DH/wA9tsP3n47\nTJQapTTuh7P74hvQyzXXhvRx4+DSS0OVXmNUPtXJkCFw1FHwjW+kOiKR9PDhh+Ex0Y8/Hn4vLroo\njKVqiN/1Jp1AqiuBNMeG9BUr4JlnYPHiVEeyewYOhF//OlRpvfYatGiR6ohEUmPNmtA2+MgjYazU\nhRfCK69Av34NG0ezrMJqbiWQO+8Mf5V07ZrqSHbfFVeEZ6j/5jepjkSkYZVXUZ19dihpvPlmaNf8\n9NMwtquhkwc08RKIqrDgyy/DEwfnz091JPWjfKqTQw8NPcrSYSbhVHMPPdRKSmDQoPSZHFPqR3VV\nVH/9a3pURzfpBKJGdLj33jBlSW5uqiOpP+VTnVx4IcydG0okzU150nj66fAyC6+2bUNHiREjwh9L\n0jitXbujiurzz8P/9dmzoX9k084mp8lXYTXnEsiGDfDgg2HakqbmpJPgRz8K7SKXXgrPP9/0/zBw\nh3ffheuuC71szjsvTIb5zDNhkO3ixaGzxPPPwz77wC9+AQUFqY5aElVSEv7tfvjD8KjnN94Ig2iX\nLg0/0y15QCNNIBMmJDY/UvzjbMs1p0b0+++HoUNh331THUk0br01TEvfp09o5+nePXze++6DTz5J\ndXT1o66kcfvtIYmahfVDhsCzz4bSSZs2cNxxYd2UKeELStLP/Plhdoi99w7/nkOGhKTxxBNhOZ0H\nPTfKkegtWsBnn8Gee9Z+zA9+AOefHxqdyl12WcjkTX1Q2qZN4Ys1Px8OOCDV0dS/zzd+zk0v38SL\nS14kKyOLrIwsMshm88Ysvl6fxYaibLIzs+jaOYvu3bLo1iWbFllZFftmZ2RXu5yVkUV2ZpX38fvW\nsq0+tmdlZAG2U/XUuefCOeeEtp9daePYti0km/vvh+XL4X//F37845Bsm6oNG0LbX7du0L59erYJ\nVVdFNWpUNKWMKEeiN7o2kNLS0Bth7dq6E0hNjejNoQTy5z+Hvz6bWvLYUryFe968h7v/czcXHXoR\nM86fgZlRUlZCSVkJxaXFlJSVsL20hAULi3njzRLefKGEBatKOGRgMQMPL+HgASW0bVdCcVlxxXHx\nx5a/tpZsrXm773hf9TzVnSt+n5q2FZeWUOolUJaJeRZZrbJo+T9ZtGqRzcMZWUzMzyLrlZqTUufW\nnenXuR/9u/anX5d+9OvSjx5te3D++cb554f2ogcegP33DyW1n/8cjjkmPb9gd0VpWRn5767kqZcK\nePmDQpZ+XUjrttvYuqYHpetz6ZiZS9dWPejRNpfcTp3Zo5uxxx4hwZS/yt9HmXBKSmDmzJA0Zs0K\n/wa33x6qY9O5lFGbRpdAykdSr1lT977NtRvvtm2hSue551IdSf0p8zImz5/M9f+6niNzj+Stn7zF\nNzrXPprwuF7wv6eE5S++CCPxp02DX/06VAcNHQqnnQZHHJGaB2tV1xB+zrnOD84u5aBDQjLZlUT0\n5eYvKVhbwNsr3mbiBxMpWFvA5uLN9OvSj/5dQlI54Yp+jLi6P29N78tFF7UnJ6dxNLq7O6s3raZw\nXSEFawv47xeF/KegkEWrCyjiY6y4Pd2z+nLQ4L5ceGA/2rduxaqNq1i+fjFLi1ay8quVvLNlFVtL\nN9HOetB6Sy7ZBT2w93IpWd+DrV/msnFVWO7aMpfu7XdONNUtJ5JwFiwISWPSJOjdO/Si+vOfd/7j\ntjFqdFVY5Z55pnLVVHV694Z//ztU5ZS7805YtQruuiuaONPBhAmhznv69FRHUj9e/+x1rpp1FWVe\nxt1D7ua4Xsft1vmKi+H110MymTYt/DFy6qkhmQwZEu0vtnsoCfztb7tfPZWI9VvXU7C2gIK1BSxe\ns5iCdeFn4bpCOrTsQLeMfmxY0o/VC/vznYH9+Pl5/Tlx4L5kZ2bXbyAJKtpSRMHaAgrXFVK4tpCC\ndQUUri2kcF0hGWTRsbQf21f1Zc3ivvTp2I8hh/dlxCn7MWhA+4Tu3ZbiLazauIpVX69i5dcrWbUx\n/CxfXrFhJSu/XsXmkk10yupBO+tBm5JcsrflYht7UFKUy9Y1Pdi0Kpeiz3LZ/lVnunWtPtFkZobv\nqZUrd1RR7b9/9PewqiirsCJPIGZ2CnAvocH+IXcfV2V7e2ASsA+QCdzl7o9Uc55KCeRPf4JLLqn5\nusXFoZ/0mjWVJw/84x/hgw/Cz6aopCTUoz76aJiAsDFbUrSE6166jjeXv8ntJ93OiINHkGH1X1T4\n5JOQbKdPD09rHDgwJJPTToMDD9z9L/WGThqJKPMyVny1oiK5zPlkMbPnF/DpxsXQbgXdW+3DYfv0\np3/XnavEbDcD3rh9Y0VSKE8WBWtDotheup2+XfrSt3NfvtGxH762L5+91485M/tStLJzgyX6qokm\nPtnEJ53NxZvZo3UPOmX1oL3l0ro0l+ytPbCNuWRs2YNBg+DgASWUUXtpsmqJssZtvgv7xl4f/uzD\nxplAzCwDKABOAlYCc4Bh7r4obp/rgfbufr2ZdQUWA93dvaTKuSolkNtugxtuqPnaH34YfkEXLaq8\n/tFH4V//CuMImqJJk0LxePbsVEeSvA1bNzD21bE8NPchrjz6Sq465iraZDfMFMKbN8PLL+8onZjt\nqOo64YTEZzJOx6SRiG3b4Mmnt3Hvo0tYvnUxRwwpoHO/xSzbHEouW0q2VKoSK1/u26Uv7Vu2rzjP\nluItfFz08U6JonBtIeu3rme/zvtVJIp+XfrRt3Nf+nbpCxu788ILxrRp8NJL6VHVWJvaEs3qTavJ\nsIzqO1HYzp0qourMMWDPAY22EX0QUOjuSwHMbDJwJhD/te5Au9hyO2Bt1eRRnbraQN5/HwYM2Hl9\nXY3o//hH+MvmxBPriiD9lJXBHXfAPfekOpLklJSVMOHdCdwy+xZO73c68y+dT492PRo0hjZtdpQ+\n3OG//w2J5He/2zGtfPn23r0rH1tT0njmmfROGvFatoSLLmjJRRccwNy5B/DAA/DM3eFLfNzPYf9D\ni/ioqDBUh60tYMqiKRXJoUPLDvTq2IuVX6/ki41f0Ltj74rkcETuEYw4eAR9O/dlr/Z7VZQky8pC\nN+VpD8NV0+Cjj+A73wnX+8Mf6u4ok2qts1vTp1Mf+nTqU/fOTVDUCWQvYFnc++WEpBLvPmCqma0E\n2gLnJXLitWtr3z5vXvilraqmRvSysjC/0mOPhQdQvfJKauord8ezz4bP19im93B3Znw0g6tnXU2P\ndj2YecFMBuxZTfZvYGbwzW+G1zXXhF59s2aFqq5bboEuXUIiOe64MOirsSaNmgwcGEqz//d/oeR+\n0UWQk9OJn/1sECNGDCIn7p+ovEps6Yal5LbLZZ8O+8S6JO9sw4ZwH6dNCx0byu/j//1fSNDZqWl+\nkSSkQy+sk4G57n6imX0DeNHMDnH3jTvvOqZiafHiPCCvxpO+/z5ceeXO66sbib5pU2jk+vzz8Mzw\n55+Hs86Ct95Kj/lmEuEeJlT79a8b15fWh198yC9n/ZLPNnzGnUPu5LS+p+12HXtUOnYMyeHcc+P+\ncp4Wntl+1FFNI2lUp1OnMKr9iitCtdL994eBjSNHhh5c/fpBhmXQs0NPenboudPx5SW56dPD/Xrv\nvR0luZtuaroDXVMlPz+f/Pz8hrmYu0f2Ao4GXoh7fx1wbZV9ngeOjXv/L+CIas7l4b9ieB15pNeo\nrMy9Wzf35ct33jZnjvvhh1de953vuF94ofvWrTvW/exn7t/7nntpac3XSScvvOD+zW82nnhXfb3K\nL556se/xuz38D2/9wbeXbE91SLILPv3U/frr3ffYw/2733X/5z/di4t3bN+82X3atPB71KuX+z77\nuF96qfvzz7tv2pSysJul8DUf0Xd8VCcOcZMJfAT0AloA84ADquxzP3BTbLk7ocqrczXncnDPznZ/\n5RX3ffet+YatWOHepUtIJFUtWOC+//473peUuLdsGf7Dx9u2zX3wYPcxY2q+Tjo57jj3xx9PdRR1\n27x9s499Zax3GdfFfznzl75u87pUhyS7YetW90mT3I85xr1nT/drrnEfOtS9XTv34493HzfOff78\n6n8XpWFEmUAircJy91IzuwyYxY5uvAvN7JLYh5oA3AY8YmYfxA67xt3X1XROs9BNtbZG9PIG9Oqq\nEqo2oi9bFvpst25deb8WLeCpp0I3zssvh86dE/rIKfHqq6Gv+bnnpjqSmiUzEFDSX8uWYbqg8pHu\nzzwTxjs8/njTGCgntYu8DcTdXwD6V1n3YNzyKkI7SELKykK7xNatsH179U+le//96hvQYedG9MLC\n0FWwOrm5oZ524kQYPTrRCBve2LGhTjorHVq0qhE/EHDS9yft9kBASU8DB4aXNB9p1qu6bmVlYYRn\nly4198SqqQsv7FwC+egj6Nu35uv99KdhZHe6Dth/550wVcLIkamOZGdLipZw7tPnMvzvw7l80OW8\n9ZO3lDxEmpBGl0Dcw2CiTt228NLiN7jv7fsoWFv5oQfz5tWcQFq3DqWX8ungP/qo5hIIwPHHhwkc\nX3+9nj5APbvjjjAVdMuWqY5khw1bN3DNi9dw5J+P5JDuh7DoskVccMgFkYwiF5HUaVy/0WMMz9zK\ndyeexOKzunDbu1fw+rLXGTJxCKs3rQZgy5bwjOCaZqE1C0mkvBRSVwIx21EKSTf//W9IbBdfnOpI\ngpKyEh6Y8wD97+vPui3rmH/pfH59/K8bbBS5iDSshBKImf3DzE6LTU2SWu2XM3vpbM5YuI7b9nmH\nJ89+kpGHjOQHT/2AbSXbmD8/NLJX1zZSLr4aq64EAmGMyNSpsK7Gpv3UuOOO0DaT6PQaUXF3phdO\n55A/HsLfF/6dmRfM5C9n/KXBR5GLSMNKtNn1AeBHwHgzexp42N0XRxdWzTK6LaZT605079KqoifW\nzSfczIK/LeDSaZdyzOqHGDCg9pFc5Q3pZWWwZEl4UH1tunaNvjHd3dlaspVNxZvYXLyZTdtjP4s3\n7bRc6qVsWt+GKR/lcPpVOfz7kzbkZOeQ0yKHNtk7lltntSYzI9oHDTSmgYAiUr8SSiDu/hLwkpl1\nAIbHlpcBfwYmuXtxhDFWjmXQH+jSugtdu+7oypthGTz2/ccY/NfBvPPp5Rx84NHM/Kgb3XK60a5F\nu/Cl2iKHFpmhWNK6Paz9CrZ/Ch27gbWA0rKWO33ZlpaVsq10GwAX/hhGj3ZG/GQLW0rCF3z8l32d\ny9UkgqrLLTJb7JQEqlvOtExeemUzPYdsYuLCms+3uXgzLbNakpOdU3EPqj133PY6940tbyvZxthX\nx/Ls4me58fgbueTwS1I2BbiIpEbCs/GaWRfgAmAkYWbdx4HBwMHunhdVgHHX97iZTLi7vbN0Kdx7\n7451yzYs4+gr76H/EZ+T2e5Lvtz0JRu3b6z4Ui0pC3M0btu2Y76d4mJo0dLZVrKNllktK+rrN23f\nxPbS7bTMaolhFceVbWtNRlkbsj2HFpZDq4w2tMnOoW2LHNq2akPHNjl0ysmhc7s2dOuYQ+d2OeRU\n8+Vcdbl1dusa5w6qasUKOOQQWLw4lI5q4u5sKdlSZ+KqdrmOJFlcVsyPB/6YG467gU6tO9UchIik\nVMqfB2Jm/ySM5ZgIPBIbu1G+7R13PyKK4KrE4Ow3g+zMLP7nzmc4tuhPzJwZpi8v5x4GL338ce1f\nrHl5MGYMFBSE+a4eeqjyl62Z0Sa7Da2zWu9UHbN1a3jecvlr9eqdl+PXbdsWYqnryWbly4k84ezK\nK0NPtKb8UCwRqR/p8Ez08e7+cnUbGiJ5VNi0B9kbDuNPp3+HGTN2Ho3+6afQrl3tyQN2TKgY34Be\nnjTq6jHUqhX07BleiYhPOFWTzccf73rC6dgxzIw6f35i1xcRiUqiCeRAM5vr7usBzKwTMNzdH4gu\ntGps6lbxQJn4NpByH3wABx9c92nKG9E/+ig84yFKySac6ko2H38clm+8MYySFxFJpUQTyMXufn/5\nG3cvMrOLCb2zGs7mblhszqrqEsjChWHuqrqUd+NNpAtvQ9vVhCMikiqJjuvItLjGADPLJMyu27BK\nWlUqgVSdymThwpoHEMbLyYGNG8Nf9OmWQEREGotEE8gLwFNmdpKZnQQ8GVvX4MoTSNu2ob1g69Yd\n2xYtSiyBtGkTSh/t2oWXiIjsukQTyLXAy8Clsde/gGuiCqo25QnErHIpxD3xBJKTEyZcVOlDRCR5\niQ4kLAP+GHulVEZcyitvB9lrr/A8jFatEntuR5s2IYGccUZ0cYqINHUJJRAz6wvcARwItCpf7+59\nIoqrllh2LMc3pCfa/gGhBFJUpBKIiMjuSLQK62FC6aMEOAF4DJhU6xERqa4EArueQEAJRERkdySa\nQFq7+78II9eXuvsY4LTowqpZfAkk/qFSCxfC/vsndo7y2Wtre5CUiIjULtEEsi02lXuhmV1mZt8H\n2kYYV0J2twRS1yy8IiJSs0QTyGigDXAFcDhhUsVRiRxoZqeY2SIzKzCza6vZfrWZzTWz98zsQzMr\nMbOONZ0vfuquZBNImzbh2I41XkVEROpSZwKJDRo8z903uvtyd/+Ru5/t7m8mcGwGcB9wMvBNYLiZ\nVapocvc73X2gux8GXA/kl0+ZUp3PP9+xXJ5AiorC1CR7711XREGvXnDqqYntKyIi1aszgbh7KWHa\n9mQMAgpj7SbFwGTgzFr2H04YpJiQ8gRS3v6R6HOM+vSBxx5L9CoiIlKdROfCmmtmU4GngU3lK939\nH3UctxewLO79ckJS2YmZtQZOAX6eYEyVEkii1VciIlI/Ek0grYC1wIlx6xyoK4Hsiu8Br9VWfdWi\nxRi2bw/P8sjLy6N37zzWrlUCEREpl5+fT35+foNcK+EnEiZ1crOjgTHufkrs/XWAu/u4avb9B/A3\nd59cw7m8c2dn3bodDekbN0L37nDCCfCTn8BZZ0X2UUREGqWUP1DKzB4mlDgqcff/qePQOcB+ZtYL\nWAUMI7RzVD1/B+DbwPm1Blsl2pwcKC2F995TCUREpKElWoX1fNxyK+D7hOei18rdS83sMmAWocH+\nIXdfaGaXhM0+IbbrWcBMd99Sa7BVoi2fUHH1ao3pEBFpaElVYcW6577m7t+q/5BqvKbvs4/z2WeV\nx4IceigUF8OCBQ0ViYhI45HyKqxq9AX2qM9AEpGZufO6Ll2gU6eGjkRERBJtA/maym0gnxOeEdKg\nqlZhQajC6tevoSMREZFEnweSFs/tq64Ecu650Lt3g4ciItLsJTQXlpl9P9ZTqvx9RzNr8E6z1ZVA\nzj4bDj+8oSMREZFEJ1O8yd03lL+JDfa7KZqQalZdAhERkdRINIFUt1+Df50rgYiIpI9EE8g7Zna3\nmX0j9robeDfKwKqjBCIikj4STSCXA9uBpwgz6m5lFyY9rC9KICIi6SPRXlibgOsijqVO1fXCEhGR\n1Ei0F9aL8U8JNLNOZjYzurCqpxKIiEj6SLQKq2v8NOvuXkQKRqIrgYiIpI9EE0iZme1T/sbMelPN\n7LxRUwIREUkfiX4l3wC8ZmazAQOOA34aWVQ1UAIREUkfiTaiv2BmRxCSxlxgClDr1OtRaJcWE6qI\niAgkPpniT4DRwN7APOBo4D9UfsRt5Fq3bsiriYhIbRJtAxkNHAksdfcTgIFAjc8uj0pGotGKiEjk\nEm1V2OruW80MM2vp7ovMrH+kkVXjqqvgm99s6KuKiEh1Ek0gy2PjQKYAL5pZEbA0urCq16+fnv0h\nIpIudvmRtmb2baAD8IK7b48kquqv68k8fldEpDmL8pG2u9yq4O6z3X1qosnDzE4xs0VmVmBm1T7F\n0MzyzGwKQ57IAAAR0ElEQVSumc03s5d3NSYREWl4u1wC2aWTm2UABcBJwEpgDjDM3RfF7dMBeAMY\n4u4rzKyru6+p5lwqgYiI7KK0KoHsokFAobsvdfdiwky+Z1bZZwTwd3dfAVBd8hARkfQTdQLZC1gW\n9355bF28fkBnM3vZzOaY2ciIYxIRkXqQDpODZAGHEQYl5gD/MbP/uPtHVXccM2ZMxXJeXh55eXkN\nFKKISOOQn59Pfn5+g1wr6jaQo4Ex7n5K7P11gLv7uLh9rgVaufvNsfd/AWa4+9+rnEttICIiu6gx\nt4HMAfYzs15m1gIYBkytss+zwGAzyzSzNsBRwMKI4xIRkd0UaRWWu5ea2WXALEKyesjdF5rZJWGz\nT4iNap8JfACUAhPc/b9RxiUiIrsv0iqs+qQqLBGRXdeYq7BERKSJUgIREZGkKIGIiEhSlEBERCQp\nSiAiIpIUJRAREUmKEoiIiCRFCURERJKiBCIiIklRAhERkaQogYiISFKUQEREJClKICIikhQlEBER\nSYoSiIiIJEUJREREkqIEIiIiSVECERGRpCiBiIhIUiJPIGZ2ipktMrMCM7u2mu3fNrP1ZvZe7PXr\nqGMSEZHdlxXlyc0sA7gPOAlYCcwxs2fdfVGVXV9x9zOijEVEROpX1CWQQUChuy9192JgMnBmNftZ\nxHGIiEg9izqB7AUsi3u/PLauqmPMbJ6ZTTOzAyOOSURE6kGkVVgJehfYx903m9mpwBSgX3U7jhkz\npmI5Ly+PvLy8hohPRKTRyM/PJz8/v0GuZe4e3cnNjgbGuPspsffXAe7u42o55hPgcHdfV2W9Rxmr\niEhTZGa4eyTNBFFXYc0B9jOzXmbWAhgGTI3fwcy6xy0PIiS1dYiISFqLtArL3UvN7DJgFiFZPeTu\nC83skrDZJwA/NLNLgWJgC3BelDGJiEj9iLQKqz6pCktEZNc15iosERFpopRAREQkKUogIiKSFCUQ\nERFJihKIiIgkRQlERESSogQiIiJJUQIREZGkKIGIiEhSlEBERKooKyujXbt2LF++vF73bWo0lYmI\nNHrt2rXDLMzWsWnTJlq2bElmZiZmxoMPPsjw4cNTHGHqRDmViRKIiDQpffr04aGHHuKEE06ocZ/S\n0lIyMzMbMKrU0VxYIiIJcneq/rF54403MmzYMEaMGEGHDh14/PHHefPNNznmmGPo1KkTe+21F6NH\nj6a0tBQICSYjI4PPPvsMgJEjRzJ69GiGDh1K+/btOfbYY1m6dOku7wswY8YM+vfvT6dOnbjiiisY\nPHgwjz32WEPcmnqnBCIizcKUKVO44IIL2LBhA+eddx7Z2dmMHz+edevW8frrrzNz5kwefPDBiv3L\nq8TKPfnkk4wdO5aioiJ69uzJjTfeuMv7rl69mvPOO4+77rqLNWvWsO+++zJnzpwIP3W0lEBEpN6Y\n7f4rKoMHD2bo0KEAtGzZksMPP5wjjzwSM6N3795cfPHFzJ49u2L/qqWYH/7whwwcOJDMzEzOP/98\n5s2bt8v7Tps2jYEDB3L66aeTmZnJlVdeSZcuXaL6yJFLh2eii0gTkc7NlD179qz0fvHixfzyl7/k\n3XffZfPmzZSWlnLUUUfVePyee+5ZsdymTRs2bty4y/uuXLlypzj23nvvXfoc6UQlEBFpFqpWM11y\nySUcfPDBLFmyhA0bNnDzzTfvVJKobz169GDZsmWV1q1YsSLSa0ZJCUREmqWvv/6aDh060Lp1axYu\nXFip/SMqp59+OnPnzmXatGmUlpZy7733smbNmsivG5XIE4iZnWJmi8yswMyurWW/I82s2Mx+EHVM\nItJ0VS1p1OSuu+7ikUceoX379lx66aUMGzasxvPUdc5E991jjz146qmnuPLKK+natSuffPIJAwcO\npGXLlgnFnG4iHQdiZhlAAXASsBKYAwxz90XV7PcisAX4q7v/o5pzaRyIiDQpZWVl5Obm8ve//51j\njz02kms05nEgg4BCd1/q7sXAZODMava7HHgGWB1xPCIiKTVz5kw2bNjAtm3buOWWW2jRogWDBg1K\ndVhJiTqB7AXEtxgtj62rYGa5wFnu/kcgwk58IiKp99prr9GnTx+6d+/Oiy++yJQpU8jOzk51WElJ\nh2689wLxbSNKIiLSZN16663ceuutqQ6jXkSdQFYA+8S93zu2Lt4RwGQLLU9dgVPNrNjdp1Y92Zgx\nYyqW8/LyyMvLq+94RUQatfz8fPLz8xvkWlE3omcCiwmN6KuAt4Hh7r6whv0fBp5TI7qISP2IshE9\n0hKIu5ea2WXALEJ7y0PuvtDMLgmbfULVQ6KMR0RE6o+mcxcRacIaczdeERFpopRARKRZW7p0KRkZ\nGZSVlQEwdOhQJk6cmNC+u+qOO+7gpz/9adKxphslEBFp9E499dRKvTTLPfvss/To0aPOL/z46Uem\nT5/OyJEjE9q3NrNnz95p5t3rr7+eCROqNv02XkogItLojRo1ikmTJu20ftKkSYwcOZKMjIb/qnP3\nhJNNY6UEIiKN3llnncXatWt57bXXKtatX7+e559/npEjRzJ9+nQOO+wwOnToQK9evbj55ptrPNcJ\nJ5zAX//6VyDMVXX11VfTrVs39ttvP6ZNm1Zp30ceeYQDDzyQ9u3bs99++1WULjZv3szQoUNZuXIl\n7dq1o3379nz++efcfPPNlUo3U6dO5aCDDqJz586ceOKJLFq0Y5rAfffdl7vuuosBAwbQqVMnhg8f\nzvbt2+vlftUXJRARafRatWrFOeecU+nZ4k899RQHHHAABx98MDk5OUycOJENGzYwbdo0/vSnPzF1\n6k5jlXcyYcIEpk+fzvvvv88777zDM888U2l79+7dmT59Ol999RUPP/wwV155JfPmzaNNmzbMmDGD\n3Nxcvv76a7766quKh0yVl0oKCgoYMWIE48eP58svv+TUU0/le9/7HiUlJRXnf/rpp5k1axaffPIJ\n77//Po888kg93K36kw5TmYhIE2E3736Vjd+UXHf9UaNGcfrpp3PffffRokULJk6cyKhRowD49re/\nXbHfQQcdxLBhw5g9ezZnnHFGred8+umn+cUvfkFubi4Q2jDiH3t76qmnViwfd9xxDBkyhFdffZVD\nDz20znj/9re/cfrpp3PiiScCcPXVV/P73/+eN954g+OPPx6A0aNH0717dwC+973vVXqMbjpQAhGR\nepPsl399OPbYY+nWrRtTpkzhiCOOYM6cOfzzn/8E4K233uL6669n/vz5bN++ne3bt3POOefUec6q\nj6Dt1atXpe0zZszglltuoaCggLKyMrZs2cIhhxySULwrV66sdD4zo2fPnpWeUFiePCA8GnfVqlUJ\nnbuhqApLRJqMkSNH8uijjzJp0iROPvlkunXrBsD555/PWWedxYoVK1i/fj2XXHJJQo+vrfoI2qVL\nl1Ysb9++nR/+8Idcc801fPnllxQVFXHqqadWnLeuBvTc3NxK5wNYtmxZo3pGuhKIiDQZF154IS+9\n9BJ/+ctfKqqvADZu3EinTp3Izs7m7bff5oknnqh0XE3J5Nxzz2X8+PGsWLGCoqIixo0bV7GtvCTT\ntWtXMjIymDFjBrNmzarY3r17d9auXctXX31V47mnTZvGyy+/TElJCXfeeSetWrXimGOO2Z1b0KCU\nQESkyejVqxff+ta32Lx5c6X2jQceeIAbb7yRDh06cNttt3HeeedVOq6mR9JefPHFnHzyyQwYMIAj\njjiCs88+u2Jb27ZtGT9+POeccw6dO3dm8uTJnHnmjufl9e/fn+HDh9OnTx86d+7M559/Xuma/fr1\nY9KkSVx22WV069aNadOm8dxzz5GVlbVTHOlKc2GJiDRhmgtLRETSjhKIiIgkRQlERESSogQiIiJJ\nUQIREZGkKIGIiEhSlEBERCQpkScQMzvFzBaZWYGZXVvN9jPM7H0zm2tmb5vZsVHHJCIiuy/SBGJm\nGcB9wMnAN4HhZrZ/ld1ecvcB7j4Q+DHwlyhjagry8/NTHULa0L3YQfdiB92LhhF1CWQQUOjuS929\nGJgMnBm/g7tvjnvbFkjuYcPNiH45dtC92EH3Ygfdi4YRdQLZC1gW9355bF0lZnaWmS0EngP+J+KY\nRESkHqRFI7q7T3H3A4CzgNtSHY+IiNQt0skUzexoYIy7nxJ7fx3g7j6ulmM+Bo5093VV1msmRRGR\nJEQ1mWLUTyScA+xnZr2AVcAwYHj8Dmb2DXf/OLZ8GNCiavKA6G6AiIgkJ9IE4u6lZnYZMItQXfaQ\nuy80s0vCZp8AnG1mFwLbgS3AuVHGJCIi9aPRPA9ERETSS1o0otelrsGIjZ2Z7W1m/zazBWb2oZld\nEVvfycxmmdliM5tpZh3ijrnezArNbKGZDYlbf5iZfRC7V/em4vPUBzPLMLP3zGxq7H2zvBdm1sHM\nno59tgVmdlQzvhdXmtn82Od43MxaNJd7YWYPmdkXZvZB3Lp6++yxezk5dsx/zGyfhAJz97R+EZLc\nR0AvIBuYB+yf6rjq+TPuCRwaW24LLAb2B8YB18TWXwv8NrZ8IDCXUAXZO3Z/ykuTbxE6IQBMB05O\n9edL8p5cCUwCpsbeN8t7ATwC/Ci2nAV0aI73AsgFlhDaSAGeAkY1l3sBDAYOBT6IW1dvnx24FHgg\ntnweMDmRuBpDCaTOwYiNnbt/7u7zYssbgYXA3oTP+Whst0cJ3ZwBziD8A5e4+6dAITDIzPYE2rn7\nnNh+j8Ud02iY2d7AUCrPStDs7oWZtQeOc/eHAWKfcQPN8F7EZAI5ZpYFtAZW0Ezuhbu/BhRVWV2f\nnz3+XM8AJyUSV2NIIAkNRmwqzKw34S+NN4Hu7v4FhCQD7BHbreo9WRFbtxfh/pRrrPfqHuD/AfEN\ndM3xXuwLrDGzh2PVeRPMrA3N8F64+0rgLuAzwufa4O4v0QzvRZw96vGzVxzj7qXAejPrXFcAjSGB\nNBtm1paQ/UfHSiJVezg0+R4PZnYa8EWsRFZb1+0mfy8IVRCHAfe7+2HAJuA6muf/i46Ev5J7Eaqz\ncszsfJrhvahFfX72hIZNNIYEsgKIb9DZO7auSYkVy58BJrr7s7HVX5hZ99j2PYHVsfUrgJ5xh5ff\nk5rWNybHAmeY2RLgSeBEM5sIfN4M78VyYJm7vxN7/3dCQmmO/y++Ayxx93Wxv5D/CXyL5nkvytXn\nZ6/YZmaZQHuvZjxeVY0hgVQMRjSzFoTBiFNTHFMU/gr8191/H7duKnBRbHkU8Gzc+mGxnhP7AvsB\nb8eKsRvMbJCZGXBh3DGNgrv/yt33cfc+hH/rf7v7SMI8aRfFdmsu9+ILYJmZ9YutOglYQDP8f0Go\nujrazFrFPsNJwH9pXvfCqFwyqM/PPjV2DoBzgH8nFFGqexck2APhFELPpELgulTHE8HnOxYoJfQw\nmwu8F/vMnYGXYp99FtAx7pjrCb0rFgJD4tYfDnwYu1e/T/Vn28378m129MJqlvcCGED4I2oe8A9C\nL6zmei9uin2uDwgNvtnN5V4ATwArgW2EZPojoFN9fXagJfC32Po3gd6JxKWBhCIikpTGUIUlIiJp\nSAlERESSogQiIiJJUQIREZGkKIGIiEhSlEBERCQpSiAiu8DMRptZq1THIZIONA5EZBeY2SfA4Z7A\nNA8iTZ1KICI1MLM2Zva8mc2NPYTnN4SJ/F42s3/F9hliZm+Y2Ttm9lRstlzM7BMzGxc77k0z65PK\nzyISBSUQkZqdAqxw94HufghwL2HSuTx3P8nMugA3ACe5+xHAu8BVcccXxY67H/g9Ik2MEohIzT4E\nvmtmd5jZYHf/isoT2h1NePrb62Y2lzA5XfzM0ZNjP58EjmmgmEUaTFaqAxBJV+5eaGaHEZ6OeKuZ\n/ZvKz1wwYJa7n1/TKeKWyyIKUyRlVAIRqYGZ9QC2uPsTwJ2EZ3F8DbSP7fImcKyZfSO2fxsz6xt3\nivNiP4cB/2mYqEUajkogIjU7GPidmZUB24FLCVVRL5jZilg7yI+AJ82sJaHE8WvClNgAnczsfWAr\nMLzhwxeJlrrxikRA3X2lOVAVlkg09JeZNHkqgYiISFJUAhERkaQogYiISFKUQEREJClKICIikhQl\nEBERSYoSiIiIJOX/A8Q831XgHn/+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f77b441668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_images, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.3)\n",
    "   \n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load submission images\n",
    "test_dir = '..\\\\test_set_images\\\\'\n",
    "submission_dir = '..\\\\submission\\\\'\n",
    "overlay_dir = '..\\\\submission_overlay\\\\'\n",
    "\n",
    "if not os.path.isdir(submission_dir):\n",
    "    os.mkdir(submission_dir)\n",
    "\n",
    "if not os.path.isdir(overlay_dir):\n",
    "    os.mkdir(overlay_dir)\n",
    "\n",
    "files = os.listdir(test_dir)\n",
    "\n",
    "for file in files:\n",
    "    img = ourProjectFunctions.load_image(test_dir + file + '\\\\' + file + '.png')\n",
    "    img_patches = ourProjectFunctions.img_crop(img, patch_size, patch_size)\n",
    "    X = np.array(img_patches)\n",
    "\n",
    "    prediction = sess.run(predict, feed_dict={ x: X, keep_prob: 1.0 })\n",
    "    img_prediction = ourProjectFunctions.label_to_img(608, 608, patch_size, patch_size, prediction)\n",
    "    \n",
    "    img_overlay = ourProjectFunctions.make_img_overlay(img, img_prediction)\n",
    "\n",
    "    save_path = file + \".png\"\n",
    "    Image.fromarray(ourProjectFunctions.img_float_to_uint8(img_prediction)).save(submission_dir + save_path)\n",
    "    Image.fromarray(ourProjectFunctions.img_float_to_uint8(img_overlay)).save(overlay_dir + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}