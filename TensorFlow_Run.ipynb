{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project Road Segmentation Using  Tensor Flow\n",
    "\n",
    "\n",
    "> We import the required python library for the homework below. Note that we have imported a python file(ourProjectFunctions) that contains some functions we would define ourselves. Those functions will be explained in the appropriate palces before it is to be used in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import ourProjectFunctions     # Python Function file with the functions we will use in the project. \n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "satImage_001.png\n",
      "Loading 100 images\n",
      "satImage_001.png\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"..\\\\training\\\\\"\n",
    "\n",
    "image_dir = root_dir + \"images\\\\\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(10000, len(files)) # Load  100 images. \n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [ourProjectFunctions.load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth\\\\\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [ourProjectFunctions.load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth and shape of Satellite  images after patching and  Linearisation: \n",
      "62500\n",
      "(62500, 16, 16, 3)\n",
      "lenth and shape of Ground truth  images after patching and Linearisation: \n",
      "62500\n",
      "(62500, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 16# each patch is 16*16 pixels\n",
    "\n",
    "img_patches = [ourProjectFunctions.img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [ourProjectFunctions.img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "# Linearize list of patches\n",
    "X = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "print (\"lenth and shape of Satellite  images after patching and  Linearisation: \")\n",
    "print(len(X))\n",
    "print(X.shape)\n",
    "print (\"lenth and shape of Ground truth  images after patching and Linearisation: \")\n",
    "print(len(gt_patches))\n",
    "print(gt_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = np.asarray([ourProjectFunctions.value_to_class_for_tensor_flow(np.mean(gt_patches[i])) for i in range(len(gt_patches))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(60500,16)\n",
      "validation_images(2000,16)\n"
     ]
    }
   ],
   "source": [
    "#img_size=10;\n",
    "VALIDATION_SIZE = 2000\n",
    "label_count = 2;\n",
    "validation_images = X[:VALIDATION_SIZE]\n",
    "validation_labels = Y[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = X[VALIDATION_SIZE:]\n",
    "train_labels = Y[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60500, 16, 16, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 16, 16, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels_count =2\n",
    "x = tf.placeholder('float', shape=[None,10,10,3])\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_count =2\n",
    "x = tf.placeholder('float', shape=[None,16,16,3])\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# (40000,784) => (40000,28,28,1)\n",
    "#image = tf.reshape(X, [-1,image_width , image_height,1])\n",
    "#print (image.get_shape()) # =>(40000,28,28,1)\n",
    "\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "print (h_pool1.get_shape()) # => (40000, 14, 14, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "#2nd convolution layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#print (h_conv2.get_shape()) # => (128000, 14,14, 64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "print (h_pool2.get_shape()) # => (1280000, 7, 7, 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Change this to code cell from menu bar to run this cell. This is if we define patch size as 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1024)\n"
     ]
    }
   ],
   "source": [
    "# densely connected layer\n",
    "W_fc1 = weight_variable([4* 4 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# (40000, 7, 7, 64) => (40000, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*64])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "print (h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "# readout layer for deep net\n",
    "labels_count =2\n",
    "W_fc2 = weight_variable([1024, labels_count])\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "print (y.get_shape()) # => (40000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = tf.argmax(y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 1000       \n",
    "    \n",
    "DROPOUT = 0.5\n",
    "#BATCH_SIZES = 200\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZES = 2000\n",
    "BATCH_SIZE= 200\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start TensorFlow session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.22 / 0.27 for step 0\n",
      "training_accuracy / validation_accuracy => 0.37 / 0.27 for step 1\n",
      "training_accuracy / validation_accuracy => 0.39 / 0.36 for step 2\n",
      "training_accuracy / validation_accuracy => 0.63 / 0.65 for step 3\n",
      "training_accuracy / validation_accuracy => 0.59 / 0.73 for step 4\n",
      "training_accuracy / validation_accuracy => 0.95 / 0.73 for step 5\n",
      "training_accuracy / validation_accuracy => 0.61 / 0.73 for step 6\n",
      "training_accuracy / validation_accuracy => 0.76 / 0.73 for step 7\n",
      "training_accuracy / validation_accuracy => 0.57 / 0.73 for step 8\n",
      "training_accuracy / validation_accuracy => 0.69 / 0.73 for step 9\n",
      "training_accuracy / validation_accuracy => 0.76 / 0.73 for step 10\n",
      "training_accuracy / validation_accuracy => 0.56 / 0.51 for step 20\n",
      "training_accuracy / validation_accuracy => 0.40 / 0.73 for step 30\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.73 for step 40\n",
      "training_accuracy / validation_accuracy => 0.65 / 0.73 for step 50\n",
      "training_accuracy / validation_accuracy => 0.79 / 0.73 for step 60\n",
      "training_accuracy / validation_accuracy => 0.40 / 0.72 for step 70\n",
      "training_accuracy / validation_accuracy => 0.76 / 0.73 for step 80\n",
      "training_accuracy / validation_accuracy => 0.55 / 0.73 for step 90\n",
      "training_accuracy / validation_accuracy => 0.69 / 0.73 for step 100\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.73 for step 200\n",
      "training_accuracy / validation_accuracy => 0.61 / 0.75 for step 300\n",
      "training_accuracy / validation_accuracy => 0.70 / 0.74 for step 400\n",
      "training_accuracy / validation_accuracy => 0.77 / 0.77 for step 500\n",
      "training_accuracy / validation_accuracy => 0.73 / 0.76 for step 600\n",
      "training_accuracy / validation_accuracy => 0.75 / 0.78 for step 700\n",
      "training_accuracy / validation_accuracy => 0.76 / 0.79 for step 800\n",
      "training_accuracy / validation_accuracy => 0.75 / 0.76 for step 900\n",
      "training_accuracy / validation_accuracy => 0.74 / 0.79 for step 999\n"
     ]
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "image_size = 400;\n",
    "\n",
    "\n",
    "\n",
    "display_step=1\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)  \n",
    "    \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_images, \n",
    "                                                            y_: validation_labels, \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_accuracy => 0.7890\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8vqrqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ce22da55c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_images, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.3)\n",
    "   \n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load submission images\n",
    "test_dir = '..\\\\test_set_images\\\\'\n",
    "submission_dir = '..\\\\submission\\\\'\n",
    "\n",
    "if not os.path.isdir(submission_dir):\n",
    "    os.mkdir(submission_dir)\n",
    "\n",
    "files = os.listdir(test_dir)\n",
    "\n",
    "for file in files:\n",
    "    img = ourProjectFunctions.load_image(test_dir + file + '\\\\' + file + '.png')\n",
    "    img_patches = ourProjectFunctions.img_crop(img, patch_size, patch_size)\n",
    "    X = np.array(img_patches)\n",
    "\n",
    "    prediction = sess.run(predict, feed_dict={ x: X, keep_prob: 1.0 })\n",
    "    img_prediction = ourProjectFunctions.label_to_img(608, 608, patch_size, patch_size, prediction)\n",
    "    \n",
    "    img_overlay = ourProjectFunctions.make_img_overlay(img, img_prediction)\n",
    "\n",
    "    save_path = submission_dir + \"submission_\" + file + \".png\"\n",
    "    Image.fromarray(ourProjectFunctions.img_float_to_uint8(img_overlay)).save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
