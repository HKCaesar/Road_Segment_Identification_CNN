{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project Road Segmentation Using  Tensor Flow\n",
    "\n",
    "\n",
    "> We import the required python library for the homework below. Note that we have imported a python file(ourProjectFunctions) that contains some functions we would define ourselves. Those functions will be explained in the appropriate palces before it is to be used in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import ourProjectFunctions     # Python Function file with the functions we will use in the project. \n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 666 # For the submission\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 800 images\n",
      "satImage_001.png\n",
      "Loading 800 images\n",
      "satImage_001.png\n"
     ]
    }
   ],
   "source": [
    "# Load a set of images\n",
    "root_dir = \"..\\\\training\\\\\"\n",
    "\n",
    "image_dir = root_dir + \"images_ext\\\\\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(10000, len(files)) # Load  100 images. \n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [ourProjectFunctions.load_image(image_dir + files[i], convert_lab=True) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth_ext\\\\\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [ourProjectFunctions.load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth and shape of Satellite  images after patching and  Linearisation: \n",
      "(2000000, 8, 8, 3)\n",
      "lenth and shape of Ground truth  images after patching and Linearisation: \n",
      "(2000000, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 8\n",
    "\n",
    "img_patches = [ourProjectFunctions.img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [ourProjectFunctions.img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "X = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "\n",
    "print (\"lenth and shape of Satellite  images after patching and  Linearisation: \")\n",
    "print(X.shape)\n",
    "print (\"lenth and shape of Ground truth  images after patching and Linearisation: \")\n",
    "print(gt_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = np.asarray([ourProjectFunctions.value_to_class_for_tensor_flow(np.mean(gt_patches[i])) for i in range(len(gt_patches))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(1999800,8)\n",
      "validation_images(200,8)\n"
     ]
    }
   ],
   "source": [
    "#img_size=10;\n",
    "VALIDATION_SIZE = 200\n",
    "label_count = 2;\n",
    "validation_images = X[:VALIDATION_SIZE]\n",
    "validation_labels = Y[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = X[VALIDATION_SIZE:]\n",
    "train_labels = Y[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999800, 8, 8, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 8, 8, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_count = 2\n",
    "data_nodes = tf.placeholder('float', shape=[None, patch_size, patch_size, 3])\n",
    "label_nodes = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed=SEED)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 32)\n"
     ]
    }
   ],
   "source": [
    "# first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# (40000,784) => (40000,28,28,1)\n",
    "#image = tf.reshape(X, [-1,image_width , image_height,1])\n",
    "#print (image.get_shape()) # =>(40000,28,28,1)\n",
    "\n",
    "\n",
    "h_conv1 = tf.nn.relu(tf.nn.bias_add(conv2d(data_nodes, W_conv1), b_conv1))\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "print (h_pool1.get_shape()) # => (40000, 14, 14, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "#2nd convolution layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 128])\n",
    "b_conv2 = bias_variable([128])\n",
    "\n",
    "h_conv2 = tf.nn.relu(tf.nn.bias_add(conv2d(h_pool1, W_conv2), b_conv2))\n",
    "#print (h_conv2.get_shape()) # => (128000, 14,14, 64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "print (h_pool2.get_shape()) # => (1280000, 7, 7, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 512)\n"
     ]
    }
   ],
   "source": [
    "# densely connected layer\n",
    "W_fc1 = weight_variable([int(patch_size/4 * patch_size/4 * 128), 512])\n",
    "b_fc1 = bias_variable([512])\n",
    "\n",
    "# (40000, 7, 7, 64) => (40000, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, int(patch_size/4 * patch_size/4 * 128)])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "print (h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# readout layer for deep net\n",
    "labels_count =2\n",
    "W_fc2 = weight_variable([512, labels_count])\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "\n",
    "y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "# cost function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, label_nodes))\n",
    "\n",
    "regularizers = tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(b_fc1) + tf.nn.l2_loss(W_fc2) + tf.nn.l2_loss(b_fc2)\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy + 5e-6 * regularizers)\n",
    "# train_step = tf.train.MomentumOptimizer(0.001, 0.0).minimize(cross_entropy)\n",
    "# train_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y),1), tf.argmax(label_nodes,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = tf.argmax(tf.nn.softmax(y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 100000\n",
    "    \n",
    "DROPOUT = 0.5\n",
    "#BATCH_SIZES = 200\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZES = 2000\n",
    "BATCH_SIZE= 200\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_validation_stats():\n",
    "    validation_accuracy = accuracy.eval(feed_dict={data_nodes: validation_images, \n",
    "                                                   label_nodes: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.3)\n",
    "   \n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions():\n",
    "    # Load submission images\n",
    "    test_dir = '..\\\\test_set_images\\\\'\n",
    "    submission_dir = '..\\\\submission\\\\'\n",
    "    overlay_dir = '..\\\\submission_overlay\\\\'\n",
    "\n",
    "    if not os.path.isdir(submission_dir):\n",
    "        os.mkdir(submission_dir)\n",
    "\n",
    "    if not os.path.isdir(overlay_dir):\n",
    "        os.mkdir(overlay_dir)\n",
    "\n",
    "    files = os.listdir(test_dir)\n",
    "\n",
    "    for file in files:\n",
    "        img = ourProjectFunctions.load_image(test_dir + file + '\\\\' + file + '.png', convert_lab=True)\n",
    "        img_patches = ourProjectFunctions.img_crop(img, patch_size, patch_size)\n",
    "        X = np.array(img_patches)\n",
    "\n",
    "        prediction = sess.run(predict, feed_dict={ data_nodes: X, keep_prob: 1.0 })\n",
    "        img_prediction = ourProjectFunctions.label_to_img(608, 608, patch_size, patch_size, prediction)\n",
    "\n",
    "        img_overlay = ourProjectFunctions.make_img_overlay(img, img_prediction)\n",
    "\n",
    "        save_path = file + \".png\"\n",
    "        Image.fromarray(ourProjectFunctions.img_float_to_uint8(img_prediction)).save(submission_dir + save_path)\n",
    "        Image.fromarray(ourProjectFunctions.img_float_to_uint8(img_overlay)).save(overlay_dir + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver to be able to restore a model\n",
    "saver = tf.train.Saver()\n",
    "save_dir = '..\\\\tmp\\\\'\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.09 / 0.10 for step 0\n",
      "training_accuracy / validation_accuracy => 0.55 / 0.17 for step 1\n",
      "training_accuracy / validation_accuracy => 0.43 / 0.47 for step 2\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.83 for step 3\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 4\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.90 for step 5\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.90 for step 6\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 7\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 8\n",
      "training_accuracy / validation_accuracy => 0.58 / 0.90 for step 9\n",
      "training_accuracy / validation_accuracy => 0.80 / 0.90 for step 10\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 20\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.90 for step 30\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.90 for step 40\n",
      "training_accuracy / validation_accuracy => 0.84 / 0.90 for step 50\n",
      "training_accuracy / validation_accuracy => 0.84 / 0.90 for step 60\n",
      "training_accuracy / validation_accuracy => 0.85 / 0.90 for step 70\n",
      "training_accuracy / validation_accuracy => 0.54 / 0.90 for step 80\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.90 for step 90\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.90 for step 100\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.90 for step 200\n",
      "training_accuracy / validation_accuracy => 0.65 / 0.87 for step 300\n",
      "training_accuracy / validation_accuracy => 0.82 / 0.90 for step 400\n",
      "training_accuracy / validation_accuracy => 0.65 / 0.90 for step 500\n",
      "training_accuracy / validation_accuracy => 0.27 / 0.90 for step 600\n",
      "training_accuracy / validation_accuracy => 0.52 / 0.94 for step 700\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.94 for step 800\n",
      "training_accuracy / validation_accuracy => 0.80 / 0.94 for step 900\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.90 for step 1000\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.90 for step 2000\n",
      "training_accuracy / validation_accuracy => 0.51 / 0.90 for step 3000\n",
      "training_accuracy / validation_accuracy => 0.91 / 0.94 for step 4000\n",
      "training_accuracy / validation_accuracy => 1.00 / 0.95 for step 5000\n",
      "training_accuracy / validation_accuracy => 0.89 / 0.93 for step 6000\n",
      "training_accuracy / validation_accuracy => 0.93 / 0.91 for step 7000\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.94 for step 8000\n",
      "training_accuracy / validation_accuracy => 0.43 / 0.90 for step 9000\n",
      "training_accuracy / validation_accuracy => 0.82 / 0.92 for step 10000\n",
      "training_accuracy / validation_accuracy => 0.82 / 0.93 for step 20000\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.94 for step 30000\n",
      "training_accuracy / validation_accuracy => 0.83 / 0.94 for step 40000\n",
      "training_accuracy / validation_accuracy => 0.81 / 0.94 for step 50000\n",
      "training_accuracy / validation_accuracy => 0.82 / 0.95 for step 60000\n",
      "training_accuracy / validation_accuracy => 0.87 / 0.95 for step 70000\n",
      "training_accuracy / validation_accuracy => 0.82 / 0.94 for step 80000\n",
      "training_accuracy / validation_accuracy => 0.85 / 0.95 for step 90000\n",
      "training_accuracy / validation_accuracy => 0.84 / 0.94 for step 99999\n",
      "validation_accuracy => 0.9450\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMBERkYT9f2VqSIRbr2fAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd84b5b6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "image_size = 400;\n",
    "\n",
    "display_step=1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "        #get new batch\n",
    "        batch_xs, batch_ys = next_batch(BATCH_SIZE)  \n",
    "\n",
    "\n",
    "        # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "        if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "\n",
    "            train_accuracy = accuracy.eval(feed_dict={data_nodes: batch_xs, \n",
    "                                                      label_nodes: batch_ys, \n",
    "                                                      keep_prob: 1.0})       \n",
    "            if(VALIDATION_SIZE):\n",
    "                validation_accuracy = accuracy.eval(feed_dict={ data_nodes: validation_images, \n",
    "                                                                label_nodes: validation_labels, \n",
    "                                                                keep_prob: 1.0})                                  \n",
    "                print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d' % \n",
    "                      (train_accuracy, validation_accuracy, i))\n",
    "\n",
    "                validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "            else:\n",
    "                 print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            x_range.append(i)\n",
    "\n",
    "            # increase display_step\n",
    "            if i%(display_step*10) == 0 and i:\n",
    "                display_step *= 10\n",
    "        \n",
    "        # train on batch\n",
    "        sess.run(train_step, feed_dict={data_nodes: batch_xs, label_nodes: batch_ys, keep_prob: DROPOUT})\n",
    "    \n",
    "    display_validation_stats()\n",
    "    \n",
    "    generate_predictions()\n",
    "    \n",
    "    saver.save(sess, '..\\\\tmp\\\\model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        generate_predictions()\n",
    "    else:\n",
    "        print('Oops, cannot load the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
